{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4_df = pd.read_csv(\"./csv-metrics/GPT_4_noContext_EvaluationMetrics_EvaluationQuestions.csv\")\n",
    "gpt_4o_df = pd.read_csv(\"./csv-metrics/GPT_4o_noContext_EvaluationMetrics_EvaluationQuestions.csv\")\n",
    "\n",
    "claude_3_df = pd.read_csv(\"./csv-metrics/Claude-3-opus_noContext_EvaluationMetrics_EvaluationQuestions.csv\")\n",
    "gemini_df = pd.read_csv(\"./csv-metrics/Gemini-1-5_noContext_EvaluationMetrics_EvaluationQuestions.csv\")\n",
    "command_df = pd.read_csv(\"./csv-metrics/Command-r-plus_noContext_EvaluationMetrics_EvaluationQuestions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4_naiveRAG_df = pd.read_csv(\"./csv-metrics/GPT_4_Context_naiveRAG_EvaluationMetrics_EvaluationQuestions.csv\")\n",
    "gpt_4o_naiveRAG_df = pd.read_csv(\"./csv-metrics/GPT_4o_Context_naiveRAG_EvaluationMetrics_EvaluationQuestions.csv\")\n",
    "\n",
    "claude_3_naiveRAG_df = pd.read_csv(\"./csv-metrics/Claude-3-opus_Context_naiveRAG_EvaluationMetrics_EvaluationQuestions.csv\")\n",
    "gemini_naiveRAG_df = pd.read_csv(\"./csv-metrics/Gemini-1-5_Context_naiveRAG_EvaluationMetrics_EvaluationQuestions.csv\")\n",
    "command_naiveRAG_df = pd.read_csv(\"./csv-metrics/Command-r-plus_Context_naiveRAG_EvaluationMetrics_EvaluationQuestions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4_advancedRAG_df = pd.read_csv(\"./csv-metrics/GPT_4_Context_advancedRAG_EvaluationMetrics_EvaluationQuestions.csv\")\n",
    "gpt_4o_advancedRAG_df = pd.read_csv(\"./csv-metrics/GPT_4o_Context_advancedRAG_EvaluationMetrics_EvaluationQuestions.csv\")\n",
    "\n",
    "claude_3_advancedRAG_df = pd.read_csv(\"./csv-metrics/Claude-3-opus_Context_advancedRAG_EvaluationMetrics_EvaluationQuestions.csv\")\n",
    "gemini_advancedRAG_df = pd.read_csv(\"./csv-metrics/Gemini-1-5_Context_advancedRAG_EvaluationMetrics_EvaluationQuestions.csv\")\n",
    "command_advancedRAG_df = pd.read_csv(\"./csv-metrics/Command-r-plus_Context_advancedRAG_EvaluationMetrics_EvaluationQuestions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# OpenAI\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'OPENAI_API_KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"\"\"\n",
    "    You are a Language Robustness Evaluator\n",
    "    Tasks:\n",
    "    1. Language Match: Check if the answer is in the expected language ({expected_language}).\n",
    "        - Yes: Return 1 (True).\n",
    "        - No: Return 0 (False).\n",
    "    2. Answer Similarity: Check if the German and English answers have similar meanings.\n",
    "        - Similar: Return 1 (True).\n",
    "        - Not Similar: Return 0 (False).\n",
    "\n",
    "    Overall Robustness:\n",
    "    - Both checks succeed (1, 1): Return 1 (High Robustness).\n",
    "    - Both checks fail (0, 0): Return 0 (Low Robustness).\n",
    "    - One check succeeds (1, 0) or (0, 1): Return 0.5 (Medium Robustness).\n",
    "\n",
    "    Question in the expected language: \n",
    "    {question}\n",
    "    Answer which should be in the expected language: \n",
    "    {llm_answer}\n",
    "    Answer which you should compare the answer to:\n",
    "    {llm_answer_test}\n",
    "\n",
    "    Print your answer like this:\n",
    "    {lanuage_match: {value}; language_similarity: {value}; language_score: {value}}\n",
    "    \"\"\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_value = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def get_evaluation_language_robustness(dataframe):\n",
    "    dataframe['language_match'] = pd.NA\n",
    "    dataframe['language_similarity'] = pd.NA\n",
    "    dataframe['language_score'] = pd.NA\n",
    "\n",
    "    for i in range(len(dataframe)):\n",
    "        question = dataframe['Question'][i]\n",
    "        llm_answer = dataframe['answer'][i]\n",
    "        expected_language = dataframe['language'][i]\n",
    "\n",
    "        if expected_language == 'german':\n",
    "            llm_answer_test = dataframe['answer'][i+1]\n",
    "        elif expected_language == 'english':\n",
    "            llm_answer_test = dataframe['answer'][i-1]\n",
    "\n",
    "        user_prompt = f\"question: {question}\\n\" \\\n",
    "                    f\"llm_answer: {llm_answer}\\n\" \\\n",
    "                    f\"llm_answer_test: {llm_answer_test}\\n\" \\\n",
    "                    f\"expected_language: {expected_language}\"        \n",
    "        print(user_prompt)\n",
    "\n",
    "        message=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}]\n",
    "        temperature=0.1\n",
    "        max_tokens=2048\n",
    "        frequency_penalty=0.0\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages = message,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            frequency_penalty=frequency_penalty\n",
    "        )\n",
    "\n",
    "        content = response.choices[0].message.content\n",
    "        # language_value.append(content)\n",
    "        print(content)\n",
    "        # Extract scores from LLM response ({lanuage_match: {value}; language_similarity: {value}; language_score: {value}})\n",
    "        language_dict = dict(item.split(\": \") for item in content.strip(\"{}\").split(\"; \"))\n",
    "        \n",
    "        language_match = float(language_dict['language_match'])\n",
    "        language_similarity = float(language_dict['language_similarity'])\n",
    "        language_score = float(language_dict['language_score'])\n",
    "\n",
    "        dataframe.at[i, 'language_match'] = language_match\n",
    "        dataframe.at[i, 'language_similarity'] = language_similarity\n",
    "        dataframe.at[i, 'language_score'] = language_score\n",
    "\n",
    "        print(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_empty_column(dataframe):\n",
    "    nan_value = float(\"NaN\") \n",
    "    dataframe.replace(\"\", nan_value, inplace=True) \n",
    "    dataframe.dropna(how='all', axis=1, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_numeric(value):\n",
    "    return float(value)\n",
    "\n",
    "# Apply the conversion function to each column\n",
    "for col in ['language_match', 'language_similarity', 'language_score']:\n",
    "  gpt_4_df[col] = gpt_4_df[col].apply(convert_to_numeric)\n",
    "\n",
    "# Print the modified DataFrame\n",
    "print(gpt_4_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "noContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "rem_empty_column(gpt_4_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_evaluation_language_robustness(gpt_4_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9285714285714286"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_4_df['language_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../llmJudge/csv-metrics/GPT_4_noContext_EvaluationMetrics_EvaluationQuestions.csv'\n",
    "gpt_4_df.to_csv(filename, sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_evaluation_language_robustness(gpt_4_naiveRAG_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8428571428571429"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_4_naiveRAG_df['language_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../llmJudge/csv-metrics/GPT_4_Context_naiveRAG_EvaluationMetrics_EvaluationQuestions.csv'\n",
    "gpt_4_naiveRAG_df.to_csv(filename, sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_evaluation_language_robustness(gpt_4_advancedRAG_df)\n",
    "\n",
    "filename = '../llmJudge/csv-metrics/GPT_4_Context_advancedRAG_EvaluationMetrics_EvaluationQuestions.csv'\n",
    "gpt_4_advancedRAG_df.to_csv(filename, sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8214285714285714"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_4_advancedRAG_df['language_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-4o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rem_empty_column(gpt_4o_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_evaluation_language_robustness(gpt_4o_df)\n",
    "\n",
    "filename = '../llmJudge/csv-metrics/GPT_4o_noContext_EvaluationMetrics_EvaluationQuestions.csv'\n",
    "gpt_4o_df.to_csv(filename, sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rem_empty_column(gpt_4o_naiveRAG_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_naiveRAG_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_evaluation_language_robustness(gpt_4o_naiveRAG_df)\n",
    "\n",
    "filename = '../llmJudge/csv-metrics/GPT_4o_Context_naiveRAG_EvaluationMetrics_EvaluationQuestions.csv'\n",
    "gpt_4o_naiveRAG_df.to_csv(filename, sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rem_empty_column(gpt_4o_advancedRAG_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_evaluation_language_robustness(gpt_4o_advancedRAG_df)\n",
    "\n",
    "filename = '../llmJudge/csv-metrics/GPT_4o_Context_advancedRAG_EvaluationMetrics_EvaluationQuestions.csv'\n",
    "gpt_4o_advancedRAG_df.to_csv(filename, sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claude 3 Opus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "noContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "rem_empty_column(claude_3_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_evaluation_language_robustness(claude_3_df)\n",
    "\n",
    "filename = '../llmJudge/csv-metrics/Claude-3-opus_noContext_EvaluationMetrics_EvaluationQuestions.csv'\n",
    "claude_3_df.to_csv(filename, sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_evaluation_language_robustness(claude_3_naiveRAG_df)\n",
    "\n",
    "filename = '../llmJudge/csv-metrics/Claude-3-opus_Context_naiveRAG_EvaluationMetrics_EvaluationQuestions.csv'\n",
    "claude_3_naiveRAG_df.to_csv(filename, sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_evaluation_language_robustness(claude_3_advancedRAG_df)\n",
    "\n",
    "filename = '../llmJudge/csv-metrics/Claude-3-opus_Context_advancedRAG_EvaluationMetrics_EvaluationQuestions.csv'\n",
    "claude_3_advancedRAG_df.to_csv(filename, sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "noContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "rem_empty_column(gemini_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_evaluation_language_robustness(gemini_df)\n",
    "\n",
    "filename = '../llmJudge/csv-metrics/Gemini-1-5_noContext_EvaluationMetrics_EvaluationQuestions.csv'\n",
    "gemini_df.to_csv(filename, sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_evaluation_language_robustness(gemini_naiveRAG_df)\n",
    "\n",
    "filename = '../llmJudge/csv-metrics/Gemini-1-5_Context_naiveRAG_EvaluationMetrics_EvaluationQuestions.csv'\n",
    "gemini_naiveRAG_df.to_csv(filename, sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_evaluation_language_robustness(gemini_advancedRAG_df)\n",
    "\n",
    "filename = '../llmJudge/csv-metrics/Gemini-1-5_Context_advancedRAG_EvaluationMetrics_EvaluationQuestions.csv'\n",
    "gemini_advancedRAG_df.to_csv(filename, sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Command R+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "noContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "rem_empty_column(command_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_evaluation_language_robustness(command_df)\n",
    "\n",
    "filename = '../llmJudge/csv-metrics/Command-r-plus_noContext_EvaluationMetrics_EvaluationQuestions.csv'\n",
    "command_df.to_csv(filename, sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_evaluation_language_robustness(command_naiveRAG_df)\n",
    "\n",
    "filename = '../llmJudge/csv-metrics/Command-r-plus_Context_naiveRAG_EvaluationMetrics_EvaluationQuestions.csv'\n",
    "command_naiveRAG_df.to_csv(filename, sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_evaluation_language_robustness(command_advancedRAG_df)\n",
    "\n",
    "filename = '../llmJudge/csv-metrics/Command-r-plus_Context_advancedRAG_EvaluationMetrics_EvaluationQuestions.csv'\n",
    "command_advancedRAG_df.to_csv(filename, sep=',', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
