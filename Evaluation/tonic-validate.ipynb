{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tonic_validate import Benchmark\n",
    "\n",
    "from tonic_validate import ValidateScorer\n",
    "from tonic_validate.metrics import AnswerConsistencyMetric, AnswerSimilarityMetric, AugmentationAccuracyMetric, AugmentationPrecisionMetric,RetrievalPrecisionMetric\n",
    "from tonic_validate import ValidateScorer\n",
    "from tonic_validate import LLMResponse\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_answer_df = pd.read_csv(\"../Evaluation Questions_Human.csv\")\n",
    "question_df = pd.read_csv(\"../Evaluation-Questions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4_df = pd.read_csv(\"../NoContext/csv-NoContext/GPT-4_EvaluationQuestions.csv\")\n",
    "gpt_4o_df = pd.read_csv(\"../NoContext/csv-NoContext/GPT-4o_EvaluationQuestions.csv\")\n",
    "\n",
    "claude_3_df = pd.read_csv(\"../NoContext/csv-NoContext/Claude-3-opus_EvaluationQuestions.csv\")\n",
    "gemini_df = pd.read_csv(\"../NoContext/csv-NoContext/Gemini-1-5_EvaluationQuestions.csv\")\n",
    "command_df = pd.read_csv(\"../NoContext/csv-NoContext/Command_r_plus_EvaluationQuestions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4_naiveRAG_question_df = pd.read_csv(\"../WithContext/NaiveRAG/csv-naiveRAG/GPT_4_context_naiveRAG_EvaluationQuestions.csv\")\n",
    "gpt_4o_naiveRAG_question_df = pd.read_csv(\"../WithContext/NaiveRAG/csv-naiveRAG/GPT_4o_context_naiveRAG_EvaluationQuestions.csv\")\n",
    "\n",
    "claude_3_naiveRAG_answer_df = pd.read_csv(\"../WithContext/NaiveRAG/csv-naiveRAG/Claude-3-opus_context_naiveRAG_EvaluationQuestions.csv\")\n",
    "gemini_naiveRAG_answer_df = pd.read_csv(\"../WithContext/NaiveRAG/csv-naiveRAG/Gemini-1-5_context_naiveRAG_EvaluationQuestions.csv\")\n",
    "command_naiveRAG_answer_df = pd.read_csv(\"../WithContext/NaiveRAG/csv-naiveRAG/Command_r_plus_context_naiveRAG_EvaluationQuestions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4_advancedRAG_question_df = pd.read_csv(\"../WithContext/AdvancedRAG/csv-advancedRAG/GPT_4_context_advancedRAG_EvaluationQuestions.csv\")\n",
    "gpt_4o_advancedRAG_question_df = pd.read_csv(\"../WithContext/AdvancedRAG/csv-advancedRAG/GPT_4o_context_advancedRAG_EvaluationQuestions.csv\")\n",
    "\n",
    "claude_3_advancedRAG_answer_df = pd.read_csv(\"../WithContext/AdvancedRAG/csv-advancedRAG/Claude-3-opus_context_advancedRAG_EvaluationQuestions.csv\")\n",
    "gemini_advancedRAG_answer_df = pd.read_csv(\"../WithContext/AdvancedRAG/csv-advancedRAG/Gemini-1-5_context_advancedRAG_EvaluationQuestions.csv\", quotechar='\"')\n",
    "command_advancedRAG_answer_df = pd.read_csv(\"../WithContext/AdvancedRAG/csv-advancedRAG/Command_r_plus_context_advancedRAG_EvaluationQuestions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    metrics = [\n",
    "        AnswerConsistencyMetric(),\n",
    "        AnswerSimilarityMetric(),\n",
    "        AugmentationAccuracyMetric(),\n",
    "        AugmentationPrecisionMetric(),\n",
    "        RetrievalPrecisionMetric(),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tonic_validate import ValidateScorer, Benchmark, LLMResponse\n",
    "\n",
    "def make_get_response(llm_answer, llm_context_list):\n",
    "    # Function to retrieve LLM answer and context from the dataframe\n",
    "    def get_llm_response(prompt):\n",
    "        return{\n",
    "            \"llm_answer\": llm_answer,\n",
    "            \"llm_context_list\": llm_context_list\n",
    "        }\n",
    "    return get_llm_response\n",
    "\n",
    "def get_evaluation(dataframe, human_dataframe, result_list):\n",
    "    for i in range(len(dataframe)):\n",
    "    # for i in range(3):\n",
    "        # Read questions and reference answers from the dataframe\n",
    "        question = dataframe['Question'][i]\n",
    "        reference_answer = human_dataframe['answer'][i]\n",
    "\n",
    "        llm_answer = dataframe['answer'][i]\n",
    "        llm_context_list = [dataframe['context'][i]]\n",
    "\n",
    "        print(question)\n",
    "        print(reference_answer)\n",
    "        print(llm_answer)\n",
    "        print(llm_context_list)\n",
    "\n",
    "        benchmark = Benchmark(\n",
    "            questions=[question],\n",
    "            answers=[reference_answer]\n",
    "        )\n",
    "\n",
    "        get_response_func = make_get_response(llm_answer, llm_context_list)\n",
    "\n",
    "        run = scorer.score(benchmark, get_response_func)\n",
    "\n",
    "        print(run.overall_scores)\n",
    "        result_list.append(run.overall_scores)\n",
    "\n",
    "        # Extract scores\n",
    "        answer_similarity_score = run.overall_scores['answer_similarity']\n",
    "        answer_consistency_score = run.overall_scores['answer_consistency']\n",
    "        augmentation_accuracy_score = run.overall_scores['augmentation_accuracy']\n",
    "        augmentation_precision_score = run.overall_scores['augmentation_precision']\n",
    "        retrieval_precision_score = run.overall_scores['retrieval_precision']\n",
    "\n",
    "        # Update Datframe\n",
    "        dataframe.at[i, 'answer_similarity'] = answer_similarity_score\n",
    "        dataframe.at[i, 'answer_consistency'] = answer_consistency_score\n",
    "        dataframe.at[i, 'augmentation_accuracy'] = augmentation_accuracy_score\n",
    "        dataframe.at[i, 'augmentation_precision'] = augmentation_precision_score\n",
    "        dataframe.at[i, 'retrieval_precision'] = retrieval_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scorer\n",
    "scorer = ValidateScorer([\n",
    "    AnswerSimilarityMetric(),\n",
    "    AnswerConsistencyMetric(),\n",
    "    AugmentationAccuracyMetric(),\n",
    "    AugmentationPrecisionMetric(),\n",
    "    RetrievalPrecisionMetric()\n",
    "], model_evaluator=\"gpt-4o\")\n",
    "# model_evaluator=\"gpt-4-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4_naive_overall_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4_naiveRAG_question_df['answer_similarity'] = pd.NA\n",
    "gpt_4_naiveRAG_question_df['answer_consistency'] = pd.NA\n",
    "gpt_4_naiveRAG_question_df['augmentation_accuracy'] = pd.NA\n",
    "gpt_4_naiveRAG_question_df['augmentation_precision'] = pd.NA\n",
    "gpt_4_naiveRAG_question_df['retrieval_precision'] = pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_evaluation(gpt_4_naiveRAG_question_df, human_answer_df, gpt4_naive_overall_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4_naiveRAG_question_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './csv-metrics/GPT_4_context_naiveRAG_EvaluationMetrics_EvaluationQuestions.csv'\n",
    "gpt_4_naiveRAG_question_df.to_csv(filename, sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4o_naive_overall_scores = []\n",
    "gpt_4o_naiveRAG_question_df['answer_similarity'] = pd.NA\n",
    "gpt_4o_naiveRAG_question_df['answer_consistency'] = pd.NA\n",
    "gpt_4o_naiveRAG_question_df['augmentation_accuracy'] = pd.NA\n",
    "gpt_4o_naiveRAG_question_df['augmentation_precision'] = pd.NA\n",
    "gpt_4o_naiveRAG_question_df['retrieval_precision'] = pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_evaluation(gpt_4o_naiveRAG_question_df, human_answer_df, gpt4o_naive_overall_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './csv-metrics/GPT_4o_context_naiveRAG_EvaluationMetrics_EvaluationQuestions.csv'\n",
    "gpt_4o_naiveRAG_question_df.to_csv(filename, sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4_advancedRAG_question_df['answer_similarity'] = pd.NA\n",
    "gpt_4_advancedRAG_question_df['answer_consistency'] = pd.NA\n",
    "gpt_4_advancedRAG_question_df['augmentation_accuracy'] = pd.NA\n",
    "gpt_4_advancedRAG_question_df['augmentation_precision'] = pd.NA\n",
    "gpt_4_advancedRAG_question_df['retrieval_precision'] = pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4_advanced_overall_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_evaluation(gpt_4_advancedRAG_question_df, human_answer_df, gpt4_advanced_overall_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './csv-metrics/GPT_4_context_advancedRAG_EvaluationMetrics_EvaluationQuestions.csv'\n",
    "gpt_4_advancedRAG_question_df.to_csv(filename, sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-4o Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_advancedRAG_question_df['answer_similarity'] = pd.NA\n",
    "gpt_4o_advancedRAG_question_df['answer_consistency'] = pd.NA\n",
    "gpt_4o_advancedRAG_question_df['augmentation_accuracy'] = pd.NA\n",
    "gpt_4o_advancedRAG_question_df['augmentation_precision'] = pd.NA\n",
    "gpt_4o_advancedRAG_question_df['retrieval_precision'] = pd.NA\n",
    "\n",
    "gpt4o_advanced_overall_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_evaluation(gpt_4o_advancedRAG_question_df, human_answer_df, gpt4o_advanced_overall_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './csv-metrics/GPT_4o_context_advancedRAG_EvaluationMetrics_EvaluationQuestions.csv'\n",
    "gpt_4o_advancedRAG_question_df.to_csv(filename, sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claude 3 Opus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_3_naiveRAG_answer_df['answer_similarity'] = pd.NA\n",
    "claude_3_naiveRAG_answer_df['answer_consistency'] = pd.NA\n",
    "claude_3_naiveRAG_answer_df['augmentation_accuracy'] = pd.NA\n",
    "claude_3_naiveRAG_answer_df['augmentation_precision'] = pd.NA\n",
    "claude_3_naiveRAG_answer_df['retrieval_precision'] = pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_naive_overall_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_evaluation(claude_3_naiveRAG_answer_df, human_answer_df, claude_naive_overall_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './csv-metrics/Claude-3-opus_context_naiveRAG_EvaluationMetrics_EvaluationQuestions.csv'\n",
    "claude_3_naiveRAG_answer_df.to_csv(filename, sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_advanced_overall_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_3_advancedRAG_answer_df['answer_similarity'] = pd.NA\n",
    "claude_3_advancedRAG_answer_df['answer_consistency'] = pd.NA\n",
    "claude_3_advancedRAG_answer_df['augmentation_accuracy'] = pd.NA\n",
    "claude_3_advancedRAG_answer_df['augmentation_precision'] = pd.NA\n",
    "claude_3_advancedRAG_answer_df['retrieval_precision'] = pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_evaluation(claude_3_advancedRAG_answer_df, human_answer_df, claude_advanced_overall_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './csv-metrics/Claude-3-opus_context_advancedRAG_EvaluationMetrics_EvaluationQuestions.csv'\n",
    "claude_3_advancedRAG_answer_df.to_csv(filename, sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Command R+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "command_naive_overall_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "command_naiveRAG_answer_df['answer_similarity'] = pd.NA\n",
    "command_naiveRAG_answer_df['answer_consistency'] = pd.NA\n",
    "command_naiveRAG_answer_df['augmentation_accuracy'] = pd.NA\n",
    "command_naiveRAG_answer_df['augmentation_precision'] = pd.NA\n",
    "command_naiveRAG_answer_df['retrieval_precision'] = pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_evaluation(command_naiveRAG_answer_df, human_answer_df, command_naive_overall_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './csv-metrics/Command-r-plus_context_naiveRAG_EvaluationMetrics_EvaluationQuestions.csv'\n",
    "command_naiveRAG_answer_df.to_csv(filename, sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "command_advanced_overall_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "command_advancedRAG_answer_df['answer_similarity'] = pd.NA\n",
    "command_advancedRAG_answer_df['answer_consistency'] = pd.NA\n",
    "command_advancedRAG_answer_df['augmentation_accuracy'] = pd.NA\n",
    "command_advancedRAG_answer_df['augmentation_precision'] = pd.NA\n",
    "command_advancedRAG_answer_df['retrieval_precision'] = pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_evaluation(command_advancedRAG_answer_df, human_answer_df, command_advanced_overall_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './csv-metrics/Command-r-plus_context_advancedRAG_EvaluationMetrics_EvaluationQuestions.csv'\n",
    "command_advancedRAG_answer_df.to_csv(filename, sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_naive_overall_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_naiveRAG_answer_df['answer_similarity'] = pd.NA\n",
    "gemini_naiveRAG_answer_df['answer_consistency'] = pd.NA\n",
    "gemini_naiveRAG_answer_df['augmentation_accuracy'] = pd.NA\n",
    "gemini_naiveRAG_answer_df['augmentation_precision'] = pd.NA\n",
    "gemini_naiveRAG_answer_df['retrieval_precision'] = pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_evaluation(gemini_naiveRAG_answer_df, human_answer_df, gemini_naive_overall_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './csv-metrics/Gemini-1-5_context_naiveRAG_EvaluationMetrics_EvaluationQuestions.csv'\n",
    "gemini_naiveRAG_answer_df.to_csv(filename, sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_advanced_overall_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_advancedRAG_answer_df['answer_similarity'] = pd.NA\n",
    "gemini_advancedRAG_answer_df['answer_consistency'] = pd.NA\n",
    "gemini_advancedRAG_answer_df['augmentation_accuracy'] = pd.NA\n",
    "gemini_advancedRAG_answer_df['augmentation_precision'] = pd.NA\n",
    "gemini_advancedRAG_answer_df['retrieval_precision'] = pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_evaluation(gemini_advancedRAG_answer_df, human_answer_df, gemini_advanced_overall_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './csv-metrics/Gemini-1-5_context_advancedRAG_EvaluationMetrics_EvaluationQuestions.csv'\n",
    "gemini_advancedRAG_answer_df.to_csv(filename, sep=',', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
